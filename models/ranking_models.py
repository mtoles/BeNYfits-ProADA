import random
from utils import *
import numpy as np
import pandas as pd
import os
from json import loads
from typing import List, Dict, Tuple, Union, Optional


class RankingModel:
    """
    Model to rank questions generaated by the CQ model based on answers generated by the oracle model.
    """

    def __init__(self):
        pass

    def forward(
        self,
        doc_summ: str,
        task: str,
        cq: list[str],
        ca: list[str],
        question: str,
        answer: str,
    ) -> str:

        # subclass this method
        return cq


class GPTClarifyingAnswersRankingModel(RankingModel):
    def __init__(self, use_cache, model_name="gpt-4-turbo-preview"):
        self.use_cache = use_cache
        self.no_answer_str = "GPT-4 did not return a valid sentence"
        self.model_name = model_name

    def forward(
        self,
        doc_summ: str,
        task: str,
        cq: list[str],
        ca: list[str],
        question: str = None,
        answer: str = None,
        temperature: float = 0.7,
    ) -> str:
        """
        Use the OpenAI API to rank clarifying questions given a summarized version of the document and the answers to clarifying questions, generated by the abstractive oracle. Return a list of selected sentences, one per question.

        Parameters:
            doc_summ (str): the summarized document
            task (str): the original task
            cq (list[str]): the clarifying questions
            ca (list[str]): answers to clarifying questions, generated by the abstractive oracle

        Returns:
            List[str]: the ordered clarifying questions, beginning with the most useful and ending with the least useful question
        """

        if question is None:
            information_list = [
                str(i + 1) + ". " + doc_summ + " " + clarifying_answer
                for i, clarifying_answer in enumerate(ca)
            ]

            nn = "\n\n"
            lm_input = f'Context: {doc_summ}\n\nTask: {task}\n\nAdditional Information:\n\n{nn.join(information_list)}\n\nReturn an ordering of these pieces of information in terms of their usefulness to the task at hand, starting with the most useful one. Return a list of indices, where each index in the list corresponds to the information piece associated with that number under Additional Information. For example, if the additional information is\n\n1. I go to highschool. I like to play guitar.\n\n2. I go to highschool. I am a boy.\n\n and the task is "What is the narrators\' gender?", then the second piece of information is more important than the first one. Therefore, return [2, 1] in this case.\n\nPlease return only a json object with only one key "response" and its value as a list with square brackets, where each element is an integer. For example, {{"response": [3, 4, 2, 1]}} is a valid json response.'

            completion = conditional_openai_call(
                x=lm_input,
                use_cache=self.use_cache,
                model=self.model_name,
                temperature=temperature,
                response_format="json",
            )
        # Tokenize the answer and return the first sentence
        # answer = nltk.sent_tokenize(
        #     loads(completion.choices[0].message.content)["answers"]
        # )
        ordering = loads(completion.choices[0].message.content)["response"]
        ordered_cq = [cq[i - 1] for i in ordering]

        return ordered_cq


class GPTPMOutputRankingModel(RankingModel):
    def __init__(self, use_cache, model_name="gpt-4-turbo-preview"):
        self.use_cache = use_cache
        self.no_answer_str = "GPT-4 did not return a valid sentence"
        self.model_name = model_name

    def forward(
        self,
        doc_full: str,
        task: str,
        # cq: list[str],
        pm_outputs: list[str],
        # gold_pm_output: str,
        # question: str = None,
        # answer: str = None,
        temperature: float = 0.0,
    ) -> str:
        """
        Use the OpenAI API to rank clarifying questions given a summarized version of the document and the answers to clarifying questions, generated by the abstractive oracle. Return a list of selected sentences, one per question.

        Parameters:
            doc_full (str): the full document
            task (str): the original task
            cq (list[str]): the clarifying questions
            ca (list[str]): answers to clarifying questions, generated by the abstractive oracle

        Returns:
            List[str]: the ordered clarifying questions, beginning with the most useful and ending with the least useful question
        """

        # if question is None:
        pm_output_list = "\n\n".join(
            [
                "Candidate Answer " + str(i + 1) + ". \n" + doc_full + " " + pm_output
                for i, pm_output in enumerate(pm_outputs)
            ]
        )

        # nn = "\n\n"
        # lm_input = f'Context: {doc_full}\n\nTask: {task}\n\nPossible Answers:\n\n{pm_output_list}\n\nReference Answer:\n\n{gold_pm_output}\n\nReturn an ordering of these answers in terms of their closeness to the reference answer, starting with the one that most closely resembles the reference answer. Return a list of indices, where each index in the list corresponds to the answer associated with that number under Possible Answers. For example, if the possible answers are\n\n1. The narrator plays guitar.\n\n2. The narrator is a boy.\n\n and the task is "What is the narrators\' gender?" when the reference answer is "The narrator\'s gender is male.", then tne second possible answer resembles the reference answer better than the first one. Therefore, return [2, 1] in this case.\n\nPlease return only a json object with only one key "response" and its value as a list with square brackets, where each element is an integer. For example, {{"response": [2, 1]}} is a valid json response.'
        # lm_input = f"Reference answer: {gold_pm_output}\n\nCandidate Answers:\n\n{pm_output_list}\n\nWhich of the candidate answers is most similar to the reference answer? Return a list of indices from most to least similar in JSON format. For example {{'response': [3, 1, 2]}}."
        lm_input = f"Task: {task} \n\nCandidate Answers:\n\n{pm_output_list}\n\nWhich of the candidate answers is most similar to the task? Return a list of indices from most to least similar in JSON format. For example {{'response': [3, 1, 2]}}."

        completion = conditional_openai_call(
            x=lm_input,
            use_cache=self.use_cache,
            model=self.model_name,
            temperature=temperature,
            response_format="json",
        )
        # Tokenize the answer and return the first sentence
        # answer = nltk.sent_tokenize(
        #     loads(completion.choices[0].message.content)["answers"]
        # )
        ordering = loads(completion.choices[0].message.content)["response"]
        # ordered_cq = [cq[i - 1] for i in ordering]

        return ordering


# def format_as_ordered_list(items: List[str], prefix: str) -> str:
#     """
#     Format a list of answers as an ordered list.

#     Parameters:
#         items (List[str]): the items to list
#         prefix (str): the prefix to use for each item

#     Returns:
#         str: the answers formatted as an ordered list
#     """
#     return "\n\n".join([f"{prefix} {i+1}. {item}" for i, item in enumerate(items)])

# testing
if __name__ == "__main__":
    doc_summ = "I go to university."

    task = "What is the narrators' gender?"

    cq = ["Do you play any musical instruments?", "Are you a boy or a girl?"]

    ca = ["I like to play guitar.", "I am a boy."]

    pm_outputs = ["The narrator plays guitar.", "The narrator is a boy."]

    gold_pm_output = "The narrator's gender is male."

    clarifying_answers_ranking_model = GPTClarifyingAnswersRankingModel(use_cache=True)

    ordered_cq = clarifying_answers_ranking_model.forward(
        doc_summ=doc_summ, task=task, cq=cq, ca=ca
    )

    assert ordered_cq == [
        "Are you a boy or a girl?",
        "Do you play any musical instruments?",
    ]

    primary_model_output_ranking_model = GPTPMOutputRankingModel(use_cache=True)

    ordered_cq = primary_model_output_ranking_model.forward(
        doc_full=doc_summ,
        task=task,
        cq=cq,
        pm_outputs=pm_outputs,
        gold_pm_output=gold_pm_output,
    )

    assert ordered_cq == [
        "Are you a boy or a girl?",
        "Do you play any musical instruments?",
    ]
