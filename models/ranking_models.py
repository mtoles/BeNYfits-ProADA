import random
from utils import *
import numpy as np
import pandas as pd
import os
from json import loads

class RankingModel:
    """
    Model to rank questions generaated by the CQ model based on answers generated by the oracle model.
    """

    def __init__(self):
        pass

    def forward(self, doc_summ: str, task: str, cq: list[str], ca: list[str], question: str, answer: str) -> str:

        # subclass this method
        return cq


class GPTRankingModel(RankingModel):
    def __init__(self, use_cache, model_name="gpt-4-1106-preview"):
        self.use_cache = use_cache
        self.no_answer_str = "GPT-4 did not return a valid sentence"
        self.model_name = model_name

    def forward(
        self,
        doc_summ: str,
        task: str,
        cq: list[str],
        ca: list[str],
        question: str=None,
        answer: str=None,
        temperature: float = 0.7
    ) -> str:
        """
        Use the OpenAI API to rank clarifying questions given a summarized version of the document and the answers to clarifying questions, generated by the abstractive oracle. Return a list of selected sentences, one per question.

        Parameters:
            doc_summ (str): the summarized document
            task (str): the original task
            cq (list[str]): the clarifying questions
            ca (list[str]): answers to clarifying questions, generated by the abstractive oracle

        Returns:
            List[str]: the ordered clarifying questions, beginning with the most useful and ending with the least useful question
        """

        if question is None:
            information_list = [str(i + 1) + '. ' + doc_summ + ' ' + clarifying_answer for i, clarifying_answer in enumerate(ca)]

            nn = "\n\n"
            lm_input = f"Context: {doc_summ}\n\nTask: {task}\n\nAdditional Information:\n\n{nn.join(information_list)}\n\nReturn an ordering of these pieces of information in terms of their usefulness to the task at hand, starting with the most useful one. Return a list of indices, where each index in the list corresponds to the information piece associated with that number under Additional Information. For example, if the additional information is\n\n1. I go to highschool. I like to play guitar.\n\n2. I go to highschool. I am a boy.\n\n and the task is \"What is the narrators' gender?\", then tne second piece of information is more important than the first one. Therfore, return [2, 1] in this case.\n\nPlease return only a json object with only one key \"response\" and its value as a list with square brackets, where each element is an integer. For example, {{\"response\": [3, 4, 2, 1]}} is a valid json response."

            print(lm_input)

            completion = conditional_openai_call(
                x=lm_input,
                use_cache=self.use_cache,
                model=self.model_name,
                temperature=temperature,
                response_format="json",
            )
        # Tokenize the answer and return the first sentence
        # answer = nltk.sent_tokenize(
        #     loads(completion.choices[0].message.content)["answers"]
        # )
        ordering = loads(completion.choices[0].message.content)['response']
        ordered_cq = [cq[i - 1] for i in ordering]

        return ordered_cq


# testing
if __name__ == "__main__":
    doc_summ = (
        "I go to university."
    )

    task = "What is the narrators' gender?"

    cq = [
        "Do you play any musical instruments?",
        "Are you a boy or a girl?"
    ]

    ca = [
        "I like to play guitar.",
        "I am a boy."
    ]


    ranking_model = GPTRankingModel(use_cache=True)

    ordered_cq = ranking_model.forward(doc_summ=doc_summ,
                               task=task,
                               cq=cq,
                               ca=ca)

    assert ordered_cq == ["Are you a boy or a girl?", "Do you play any musical instruments?"]
