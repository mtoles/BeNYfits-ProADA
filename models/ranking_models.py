import random
from utils import *
import numpy as np
import pandas as pd
import os
from json import loads
from typing import List, Dict, Tuple, Union, Optional

np.random.seed(0)

class RankingModel:
    """
    Model to rank questions generaated by the CQ model based on answers generated by the oracle model.
    """

    def __init__(self):
        pass

    def forward(
        self,
        doc_summ: str,
        task: str,
        cq: list[str],
        ca: list[str],
        question: str,
        answer: str,
    ) -> str:

        # subclass this method
        return cq


class GPTClarifyingAnswersRankingModel(RankingModel):
    def __init__(self, use_cache, model_name="gpt-4-turbo-preview"):
        self.use_cache = use_cache
        self.no_answer_str = "GPT-4 did not return a valid sentence"
        self.model_name = model_name

    def forward(
        self,
        doc_summ: str,
        task: str,
        cq: list[str],
        ca: list[str],
        question: str = None,
        answer: str = None,
        temperature: float = 0.7,
    ) -> str:
        """
        Use the OpenAI API to rank clarifying questions given a summarized version of the document and the answers to clarifying questions, generated by the abstractive oracle. Return a list of selected sentences, one per question.

        Parameters:
            doc_summ (str): the summarized document
            task (str): the original task
            cq (list[str]): the clarifying questions
            ca (list[str]): answers to clarifying questions, generated by the abstractive oracle

        Returns:
            List[str]: the ordered clarifying questions, beginning with the most useful and ending with the least useful question
        """

        if question is None:
            information_list = [
                str(i + 1) + ". " + doc_summ + " " + clarifying_answer
                for i, clarifying_answer in enumerate(ca)
            ]

            nn = "\n\n"
            lm_input = f'Context: {doc_summ}\n\nTask: {task}\n\nAdditional Information:\n\n{nn.join(information_list)}\n\nReturn an ordering of these pieces of information in terms of their usefulness to the task at hand, starting with the most useful one. Return a list of indices, where each index in the list corresponds to the information piece associated with that number under Additional Information. For example, if the additional information is\n\n1. I go to highschool. I like to play guitar.\n\n2. I go to highschool. I am a boy.\n\n and the task is "What is the narrators\' gender?", then the second piece of information is more important than the first one. Therefore, return [2, 1] in this case.\n\nPlease return only a json object with only one key "response" and its value as a list with square brackets, where each element is an integer. For example, {{"response": [3, 4, 2, 1]}} is a valid json response.'

            completion = conditional_openai_call(
                x=lm_input,
                use_cache=self.use_cache,
                model=self.model_name,
                temperature=temperature,
                response_format="json",
            )
        # Tokenize the answer and return the first sentence
        # answer = nltk.sent_tokenize(
        #     loads(completion.choices[0].message.content)["answers"]
        # )
        ordering = loads(completion.choices[0].message.content)["response"]
        ordered_cq = [cq[i - 1] for i in ordering]

        return ordered_cq


class GPTPMOutputRankingModel(RankingModel):
    def __init__(self, use_cache, model_name="gpt-4-turbo-preview"):
        self.use_cache = use_cache
        self.no_answer_str = "GPT-4 did not return a valid sentence"
        self.model_name = model_name

    def forward(
        self,
        doc_full: str,
        task: str,
        # cq: list[str],
        pm_outputs: list[str],
        order: list[int],
        # gold_pm_output: str,
        # question: str = None,
        # answer: str = None,
        temperature: float = 0.0,
    ) -> str:
        """
        Use the OpenAI API to rank clarifying questions given a summarized version of the document and the answers to clarifying questions, generated by the abstractive oracle. Return a list of selected sentences, one per question.

        Parameters:
            doc_full (str): the full document
            task (str): the original task
            cq (list[str]): the clarifying questions
            ca (list[str]): answers to clarifying questions, generated by the abstractive oracle

        Returns:
            List[str]: the ordered clarifying questions, beginning with the most useful and ending with the least useful question
        """

        # if question is None:
        pm_output_list = "\n\n".join(
            [
                "Candidate Answer " + str(i + 1) + ". \n" + pm_output
                for i, pm_output in enumerate(pm_outputs)
            ]
        )

        # lm_input = f"Task: {task}\n\nContext:{doc_full}\n\nCandidate Answers:\n\n{pm_output_list}\n\nWhich of the candidate answers best answers the task? Return a list of indices from worst to best in JSON format. For example {{'response': [3, 1, 2]}}." # ranking prompt
        lm_input = f"Task: {task}\n\nContext:{doc_full}\n\nCandidate Answers:\n\n{pm_output_list}\n\nWhich of the candidate answers is more helpful in answering the task? Return your answer in json, e.g. {{'answer': 1}}."

        completion = conditional_openai_call(
            x=lm_input,
            use_cache=self.use_cache,
            model=self.model_name,
            temperature=temperature,
            response_format="json",
        )

        # nominal_ranking = [
        #     x - 1 for x in loads(completion.choices[0].message.content)["response"]
        # ]
        # undo shuffling based on the `ordering` parameter
        # TODO: retry if the ordering is not valid
        # assert len(nominal_ranking) == len(pm_outputs)
        # assert set(nominal_ranking) == set(range(len(pm_outputs)))
        # ordered_cq = [cq[i - 1] for i in ordering]
        # ranking_of_each_candidate = [-1] * len(pm_outputs)
        # for i, j in enumerate(order):
        #     ranking_of_each_candidate[i] = nominal_ranking[j]

        best_candidate = loads(completion.choices[0].message.content)["answer"] - 1
        if best_candidate == 0:
            ranking_of_each_candidate = [1, 0]
        else:
            ranking_of_each_candidate = [0, 1]
        return ranking_of_each_candidate

    # remap the preferences to the original order
    def reconstruct(shuffled_list, order):
        reconstructed_list = []
        for i in range(len(order)):
            j = order.index(i)
            reconstructed_list.append(shuffled_list[j])
        return reconstructed_list


# def format_as_ordered_list(items: List[str], prefix: str) -> str:
#     """
#     Format a list of answers as an ordered list.

#     Parameters:
#         items (List[str]): the items to list
#         prefix (str): the prefix to use for each item

#     Returns:
#         str: the answers formatted as an ordered list
#     """
#     return "\n\n".join([f"{prefix} {i+1}. {item}" for i, item in enumerate(items)])

class GPTPMPairwiseRankingModel():
    def __init__(self, use_cache, model_name="gpt-4-turbo-preview"):
        self.use_cache = use_cache
        self.model_name = model_name 
    def forward(
        self,
        task: str,
        doc_full: str,
        doc1: str,
        doc2: str,
        temperature: float = 0.0,
    ) -> int:
        is_reversed = np.random.choice([0, 1])
        if is_reversed: 
            doc1, doc2 = doc2, doc1
        lm_input = f"Task: {task}\n\nContext:\n\n{doc_full}\n\nCandidate Answer 1:\n\n{doc1}\n\nCandidate Answer 2:\n\n{doc2}\n\nWhich of the candidate answers is more helpful in answering the task? If answers are identical, choose one at random. Return your answer in json, e.g. {{'answer': 1}}."
        completion = conditional_openai_call(
            x=lm_input,
            use_cache=self.use_cache,
            model=self.model_name,
            temperature=temperature,
            response_format="json",
        )
        lm_output = loads(completion.choices[0].message.content)["answer"]
        try:
            lm_output = int(lm_output)
        except ValueError:
            print("Ranking at Random")
            lm_output = np.random.choice([0, 1]) # if gpt4 refuses to rank, pick one at random
        lm_output -= 1 # since prompt is 1-based
        if is_reversed:
            return 1 - lm_output
        return lm_output
 

# testing
if __name__ == "__main__":
    doc_summ = "I go to university."

    task = "What is the narrators' gender?"

    cq = ["Do you play any musical instruments?", "Are you a boy or a girl?"]

    ca = ["I like to play guitar.", "I am a boy."]

    pm_outputs = ["The narrator plays guitar.", "The narrator is a boy."]

    gold_pm_output = "The narrator's gender is male."

    clarifying_answers_ranking_model = GPTClarifyingAnswersRankingModel(use_cache=True)

    ordered_cq = clarifying_answers_ranking_model.forward(
        doc_summ=doc_summ, task=task, cq=cq, ca=ca
    )

    assert ordered_cq == [
        "Are you a boy or a girl?",
        "Do you play any musical instruments?",
    ]

    primary_model_output_ranking_model = GPTPMOutputRankingModel(use_cache=True)

    ordered_cq = primary_model_output_ranking_model.forward(
        doc_full=doc_summ,
        task=task,
        cq=cq,
        pm_outputs=pm_outputs,
        gold_pm_output=gold_pm_output,
    )

    assert ordered_cq == [
        "Are you a boy or a girl?",
        "Do you play any musical instruments?",
    ]
