from utils import *
from typing import List, Dict, Tuple, Union, Optional
from json import loads
import nltk
from tqdm import tqdm
import os
import torch
import transformers
from huggingface_hub import login

nltk.download("punkt")
nn = "\n\n"


class OracleModel:
    """
    Model to answer questions generated by the primary model. Answers are sentences selected from the original document. Subclass this model.
    """

    def __init__(self):
        self.main_instruction = "Use the context to answer the question. Use only the information given in context and do not add any additional information. Answer the question in the first person, as if you are the original writer of the Reddit post. Do not add any additional information beyond what is in the context. If you cannot answer the question from the context, respond with 'Sorry, I'm not sure.'"

    def forward_list(self, document: str, question: str) -> str:
        # subclass this method
        raise NotImplementedError
        return self.split_doc_to_sentences(document)[0]

class GPTOracleAbstractiveModel(OracleModel):
    def __init__(self, model_name, use_cache):
        super().__init__()
        self.model_name = model_name
        self.use_cache = use_cache

    def lm_input_template(self, document, question):
        json_instruction = (
            "Return the answer in JSON form, i.e. {{'answer': 'the answer here'}}."
            if "gpt" in self.model_name.lower()
            else ""
        )
        return f"Context: {document}\n\n{self.main_instruction} {json_instruction}\n\nQuestion:\n\n{question}"

    def forward_list(
        self,
        document: str,
        questions: List[str],
        temperature: float = 0.7,
    ) -> str:
        answers = []
        for question in questions:
            lm_input = self.lm_input_template(document, question)
            completion = conditional_openai_call(
                x=lm_input,
                use_cache=self.use_cache,
                model=self.model_name,
                temperature=temperature,
                response_format="json",
            )
            answers.append(loads(completion.choices[0].message.content)["answer"])
        return answers

    # def forward_single()


class Llama3OracleModel(OracleModel):
    """
    Llama3 Oracle Model.
    """

    def __init__(self, model_size, batch_size):
        super().__init__()
        # self.llama_system_prompt = 'Based on the context provided, answer the specific question listed using only the information from the context. Do not add any additional information beyond what is in the context. If you cannot answer the question from the context, respond with "Sorry, I\'m not sure." Respond in the first person, as if you are the original writer of the content. Return your answer as a simple string, directly addressing the question without including any additional text. \n\n Context: {user_input}'

        if model_size == "8b":
            self.model_name = "meta-llama/Meta-Llama-3-8B-Instruct"
        elif model_size == "70b":
            self.model_name = "meta-llama/Meta-Llama-3-70B-Instruct"
        else:
            raise ValueError(f"Unknown llama model size {model_size}")
        # self.system_prompt = self.llama_system_prompt

        self.hf_api_key = os.getenv("HUGGINGFACE_API_KEY")
        login(token=self.hf_api_key)

        self.pipeline = transformers.pipeline(
            "text-generation",
            model=self.model_name,
            model_kwargs={"torch_dtype": torch.bfloat16},
            device_map="auto",
        )

        # self.system_prompt = "Based on the context provided, answer the specific question listed using only the information from the context. Do not add any additional information beyond what is in the context. Respond in the first person, as if you are the original writer of the content. Return your answer as a simple string, directly addressing the question without including any additional text. \n\n Context: {user_input}"
        self.user_prompt = "{question_string}"

        self.batch_size = batch_size

    def forward_batch(self, documents: List[str], questions: List[str]) -> List[str]:
        formatted_user_messages = [
            [
                {
                    "role": "system",
                    "content": f"{self.main_instruction}\n\nContext:\n\n{doc}",
                },
                {
                    "role": "user",
                    "content": self.user_prompt.format(question_string=question),
                },
            ]
            for doc, question in zip(documents, questions)
        ]

        llama_formatted_prompts = [
            self.pipeline.tokenizer.apply_chat_template(
                prompt, tokenize=False, add_generation_prompt=True
            )
            for prompt in formatted_user_messages
        ]

        sequences = self.pipeline(llama_formatted_prompts)

        outputs = []
        for seq, llama_formatted_prompt in zip(sequences, llama_formatted_prompts):
            llama_parsed_output = seq[0]["generated_text"]
            llama_parsed_output = llama_parsed_output[len(llama_formatted_prompt) :]
            llama_parsed_output = llama_parsed_output.strip()

            outputs.append(llama_parsed_output)

        return outputs

    def forward(self, documents: List[str], questions: List[str]) -> List[str]:
        assert len(documents) == len(
            questions
        ), "The length of the documents list must be equal to the length of the questions list."

        results = []
        n_batches = len(documents) // self.batch_size + (
            0 if len(documents) % self.batch_size == 0 else 1
        )

        for i in tqdm(range(n_batches)):
            batch_documents = documents[i * self.batch_size : (i + 1) * self.batch_size]
            batch_questions = questions[i * self.batch_size : (i + 1) * self.batch_size]

            batch_results = self.forward_batch(batch_documents, batch_questions)
            results.extend(batch_results)

        return results

    def forward_list(self, document: str, questions: List[str]) -> List[str]:
        return self.forward([document] * len(questions), questions)


# testing
if __name__ == "__main__":
    document = (
        "My name is Matt. I wrote this code. I am a student at Columbia University."
    )
    question1 = "What is my name?"
    question2 = "What did I write?"
    question3 = "Where do I go to school?"

    model = GPTOracleAbstractiveModel(use_cache=False)
    print(model.forward(document, [question1], 0.7))
    print(model.forward(document, [question2], 0.7))
    print(model.forward(document, [question3], 0.7))
    print(model.forward(document, [question1, question2, question3], 0.7))
    abs_model = GPTOracleAbstractiveModel(use_cache=False)
    print(abs_model.forward(document, [question1, question2, question3], 0.7))
