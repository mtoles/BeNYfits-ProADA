import random
from utils import *
import numpy as np
from typing import List, Dict, Tuple, Union, Optional
import pandas as pd
import os
from json import loads
import nltk
nltk.download('punkt')

class OracleModel:
    """
    Model to answer questions generated by the primary model. Answers are sentences selected from the original document. Subclass this model.
    """

    def __init__(self):
        pass

    def forward(self, document: str, question: str) -> str:
        # subclass this method
        return self.split_doc_to_sentences(document)[0]


class GPTOracleModel(OracleModel):
    def __init__(self, use_cache):
        self.use_cache = use_cache
        self.no_answer_str = "GPT-4 did not return a valid sentence"

    def forward(
        self,
        document: str,
        questions: List[str],
        temperature: float = 0.7,
        model="gpt-4-1106-preview",
    ) -> str:
        """
        Use the OpenAI API to answer questions given a document. Return a list of selected sentences, one per question.

        Parameters:
            document (str): the full document
            questions (List[str]): the questions
            temperature (float): the temperature to use for the GPT model
            model (str): the name of the OpenAI model to use

        Returns:
            List[str]: the selected sentence
        """
        nn = "\n\n"
        lm_input = f"Context: {document}\n\nQuestions:\n\n{nn.join(questions)}\n\nFor each question, return a single sentence, verbatim, from the context, that answers the question. If no sentence from the context answers the question or the question cannot be answered confidently, return 'Question not answerable'. Return the sentences sentences together in a JSON list, as in {{'answers': ['The first answer', 'The second answer']}}"
        completion = conditional_openai_call(
            x=lm_input,
            use_cache=self.use_cache,
            model=model,
            temperature=temperature,
            response_format="json",
        )
        # Tokenize the answer and return the first sentence
        # answer = nltk.sent_tokenize(
        #     loads(completion.choices[0].message.content)["answers"]
        # )[0]
        answers = loads(completion.choices[0].message.content)["answers"]
        answers = [nltk.sent_tokenize(answer)[0] for answer in answers]
        # Check that the answer is actually a sentence in the document
        actual_answers = []
        for a in answers:
            if (a.lower() in document.lower()) or (
                a.lower() == "question not answerable"
            ):
                actual_answers.append(a)
            else:
                actual_answers.append(self.no_answer_str)
        return actual_answers

<<<<<<< HEAD
class GPTOracleAbstractiveModel(OracleModel):
    def __init__(self, use_cache):
        self.use_cache = use_cache

    def forward(
        self,
        document: str,
        questions: List[str],
        temperature: float,
        model="gpt-4-1106-preview",
    ) -> str:
        """
        Use the OpenAI API to answer questions given a document. Return a list of selected sentences, one per question.

        Parameters:
            document (str): the full document
            questions (List[str]): the questions
            temperature (float): the temperature to use for the GPT model
            model (str): the name of the OpenAI model to use

        Returns:
            List[str]: the selected sentence
        """
        nn="\n\n"
        lm_input = f"Context: {document}\n\nQuestions:{nn.join(questions)}\n\nUse the context to answer the questions. Use only the information given in context and do not add any additional information. Return only one answer per question together in a JSON list with key as 'answers'."
        completion = conditional_openai_call(
            x=lm_input,
            use_cache=self.use_cache,
            model=model,
            temperature=temperature,
            response_format="json",
        )
        answers = loads(completion.choices[0].message.content)["answers"]
        answers = [nltk.sent_tokenize(answer)[0] for answer in answers]
        return answers
=======
>>>>>>> main

# testing
if __name__ == "__main__":
    document = (
        "My name is Matt. I wrote this code. I am a student at Columbia University."
    )
    question1 = "What is my name?"
    question2 = "What did I write?"
    question3 = "Where do I go to school?"
<<<<<<< HEAD

    model = GPTOracleModel(use_cache=False)
    print(model.forward(document, [question1], 0.7))
    print(model.forward(document, [question2], 0.7))
    print(model.forward(document, [question3], 0.7))

    abs_model = GPTOracleAbstractiveModel(use_cache=False)
    print(abs_model.forward(document, [question1, question2, question3], 0.7))
=======
    model = GPTOracleModel(use_cache=False)
    print(model.forward(document, question1, 0.7))
    print(model.forward(document, question2, 0.7))
    print(model.forward(document, question3, 0.7))
>>>>>>> main
