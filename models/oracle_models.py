from utils import *
from typing import List, Dict, Tuple, Union, Optional
from json import loads
from tqdm import tqdm
import os
import torch
import transformers
from huggingface_hub import login
from lmwrapper.structs import LmPrompt
from lmwrapper.batch_config import CompletionWindow
from utils import ModelFamily

nn = "\n\n"


class OracleModel:
    """
    Model to answer questions generated by the primary model. Answers are sentences selected from the original document. Subclass this model.
    """

    def __init__(self):
        self.main_instruction = "Use the context to answer the question. Use only the information given in context and do not add any additional information. Answer the question in the first person, as if you are the original writer of the Reddit post. Do not add any additional information beyond what is in the context. If you cannot answer the question from the context, respond with 'Sorry, I'm not sure.'"

    def forward_list(self, document: str, question: str) -> str:
        # subclass this method
        raise NotImplementedError
        return self.split_doc_to_sentences(document)[0]


class GPTOracleAbstractiveModel(OracleModel):
    def __init__(self, model_name, use_cache):
        super().__init__()
        self.model_name = model_name
        self.use_cache = use_cache

    def lm_input_template(self, document, question):
        json_instruction = (
            "Return the answer in JSON form, i.e. {{'answer': 'the answer here'}}."
            if "gpt" in self.model_name.lower()
            else ""
        )
        return f"Context: {document}\n\n{self.main_instruction} {json_instruction}\n\nQuestion:\n\n{question}"

    def forward_list(
        self,
        document: str,
        questions: List[str],
        temperature: float = 0.0,
    ) -> str:
        answers = []
        for question in questions:
            answers.append(self.forward_single(document, question, temperature))
        return answers

    def forward_single(
        self,
        document: str,
        question: str,
        temperature: float = 0.0,
    ) -> str:
        lm_input = self.lm_input_template(document, question)
        completion = conditional_openai_call(
            x=lm_input,
            use_cache=self.use_cache,
            model=self.model_name,
            temperature=temperature,
            response_format="json",
        )
        return str(loads(completion.choices[0].message.content)["answer"])

    def forward_batch(
        self,
        documents: List[str],
        questions: List[str],
        temperature: float = 0.0,
    ) -> List[str]:
        lm_inputs = [
            self.lm_input_template(doc, question)
            for doc, question in zip(documents, questions)
        ]
        completions = []
        for lmi in lm_inputs:
            completions.append(
                conditional_openai_call(
                    x=lmi,
                    use_cache=self.use_cache,
                    model=self.model_name,
                    temperature=temperature,
                    response_format="json",
                )
                .choices[0]
                .message.content
            )

        outputs = [str(loads(c)["answer"]) for c in completions]
        return outputs


class Llama3OracleModel(OracleModel):
    """
    Llama3 Oracle Model.
    """

    def __init__(self, model_name, batch_size, pipeline=None):
        super().__init__()
        # self.llama_system_prompt = 'Based on the context provided, answer the specific question listed using only the information from the context. Do not add any additional information beyond what is in the context. If you cannot answer the question from the context, respond with "Sorry, I\'m not sure." Respond in the first person, as if you are the original writer of the content. Return your answer as a simple string, directly addressing the question without including any additional text. \n\n Context: {user_input}'

        # if model_size == "8b":
        #     self.model_name = "meta-llama/Meta-Llama-3-8B-Instruct"
        # elif model_size == "70b":
        #     self.model_name = "meta-llama/Meta-Llama-3-70B-Instruct"
        # else:
        #     raise ValueError(f"Unknown llama model size {model_size}")
        # self.system_prompt = self.llama_system_prompt
        self.model_name = model_name

        self.hf_api_key = os.getenv("HUGGINGFACE_API_KEY")
        login(token=self.hf_api_key)
        
        if pipeline:
            self.pipeline = pipeline
        else:
            self.pipeline = transformers.pipeline(
                "text-generation",
                model=self.model_name,
                model_kwargs={"torch_dtype": torch.bfloat16},
                device_map="auto",
            )

        # self.system_prompt = "Based on the context provided, answer the specific question listed using only the information from the context. Do not add any additional information beyond what is in the context. Respond in the first person, as if you are the original writer of the content. Return your answer as a simple string, directly addressing the question without including any additional text. \n\n Context: {user_input}"
        self.user_prompt = "{question_string}"

        self.batch_size = batch_size

    def forward_batch(self, documents: List[str], questions: List[str]) -> List[str]:
        formatted_user_messages = [
            [
                {
                    "role": "system",
                    "content": f"{self.main_instruction}\n\nContext:\n\n{doc}",
                },
                {
                    "role": "user",
                    "content": self.user_prompt.format(question_string=question),
                },
            ]
            for doc, question in zip(documents, questions)
        ]

        llama_formatted_prompts = [
            self.pipeline._tokenizer.apply_chat_template(
                prompt, tokenize=False, add_generation_prompt=True
            )
            for prompt in formatted_user_messages
        ]
        sequences = self.pipeline.predict_many(
            ([LmPrompt(p, cache=False) for p in llama_formatted_prompts]),
            completion_window=CompletionWindow.ASAP,
        )

        # sequences = self.pipeline(llama_formatted_prompts, self.batch_size, pad_token_id=self.pipeline.tokenizer.eos_token_id)

        # outputs = []
        # outputs.extend([x.completion_text for x in sequences])
        outputs = [x.completion_text for x in sequences]
        # for seq, llama_formatted_prompt in zip(sequences, llama_formatted_prompts):
        #     llama_parsed_output = seq[0]["generated_text"]
        #     llama_parsed_output = llama_parsed_output[len(llama_formatted_prompt) :]
        #     llama_parsed_output = llama_parsed_output.strip()

        #     outputs.append(llama_parsed_output)

        return outputs

    def forward(self, documents: List[str], questions: List[str]) -> List[str]:
        assert len(documents) == len(
            questions
        ), "The length of the documents list must be equal to the length of the questions list."

        results = []
        # n_batches = len(documents) // self.batch_size + (
        #     0 if len(documents) % self.batch_size == 0 else 1
        # )

        # for i in tqdm(range(n_batches)):
        #     batch_documents = documents[i * self.batch_size : (i + 1) * self.batch_size]
        #     batch_questions = questions[i * self.batch_size : (i + 1) * self.batch_size]

        batch_results = self.forward_batch(documents, questions)
        results.extend(batch_results)

        return results

    def forward_list(self, document: str, questions: List[str]) -> List[str]:
        return self.forward([document] * len(questions), questions)

# TODO - RATTAN - Only this class remains rest removed
class BaseOracleModel:
    def __init__(self, lm_wrapper, batch_size):
        super().__init__()

        self.main_instruction = "Use the context to answer the question. Use only the information given in context and do not add any additional information. Answer the question in the first person, as if you are the original writer of the Reddit post. Do not add any additional information beyond what is in the context. If you cannot answer the question from the context, respond with 'Sorry, I'm not sure.'"

        self.lm_wrapper = lm_wrapper
        self.batch_size = batch_size

        self.hf_api_key = os.getenv("HUGGINGFACE_API_KEY")
        login(token=self.hf_api_key)

    def _format_llama_prompt(self, document: str, question: str) -> str:
        user_prompt = "{question_string}"
        formatted_user_messages = [
            {
                "role": "system",
                "content": f"{self.main_instruction}\n\nContext:\n\n{document}",
            },
            {
                "role": "user",
                "content": user_prompt.format(question_string=question),
            },
        ]
        return self.lm_wrapper.language_model._tokenizer.apply_chat_template(
            formatted_user_messages, tokenize=False, add_generation_prompt=True
        )

    def _format_gpt_prompt(self, document: str, question: str) -> str:
        json_instruction = "Return the answer in JSON form, i.e. {{'answer': 'the answer here'}}."
        return f"Context: {document}\n\n{self.main_instruction} {json_instruction}\n\nQuestion: {question}\n\nAnswer:"

    def _format_gemma_prompt(self, document: str, question: str) -> str:
        return f"<start_of_turn>system\nUse the following context to answer the user's question: {document}\n<end_of_turn>\n<start_of_turn>user\n{question}\n<end_of_turn>\n<start_of_turn>model\n"

    def _format_mistral_prompt(self, document: str, question: str) -> str:
        return f"[INST] Context: {document}\n\n{self.main_instruction}\n\nQuestion: {question} [/INST]"

    def _format_default_prompt(self, document: str, question: str) -> str:
        json_instruction = "Return the answer in JSON form, i.e. {{'answer': 'the answer here'}}."
        return f"Context: {document}\n\n{self.main_instruction} {json_instruction}\n\nQuestion:\n\n{question}"

    def forward_batch(self, documents: List[str], questions: List[str]) -> List[str]:
        format_func = {
            ModelFamily.LLAMA: self._format_llama_prompt,
            ModelFamily.GPT: self._format_gpt_prompt,
            ModelFamily.GEMMA: self._format_gemma_prompt,
            ModelFamily.MISTRAL: self._format_mistral_prompt
        }.get(self.lm_wrapper.family, self._format_default_prompt)

        formatted_prompts = [
            format_func(doc, question)
            for doc, question in zip(documents, questions)
        ]

        sequences = self.lm_wrapper.language_model.predict_many(
            ([LmPrompt(p, cache=False) for p in formatted_prompts]),
            completion_window=CompletionWindow.ASAP,
        )

        outputs = [x.completion_text for x in sequences]
        return outputs
    
# testing
if __name__ == "__main__":
    document = (
        "My name is Matt. I wrote this code. I am a student at Columbia University."
    )
    question1 = "What is my name?"
    question2 = "What did I write?"
    question3 = "Where do I go to school?"

    # model = GPTOracleAbstractiveModel(use_cache=False)
    # print(model.forward(document, [question1], 0.7))
    # print(model.forward(document, [question2], 0.7))
    # print(model.forward(document, [question3], 0.7))
    # print(model.forward(document, [question1, question2, question3], 0.7))
    # abs_model = GPTOracleAbstractiveModel(use_cache=False)
    # print(abs_model.forward(document, [question1, question2, question3], 0.7))
